{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model lowercase characters and score NIST dataset\n",
    "    - Create a model only with lowercase charactes of the new character database. \n",
    "    - Evaluate it in a test partition\n",
    "    - Use the previous trained model to score the lowercase characters of NIST test database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path = '/home/jorge/data/tesis/handwriting/databases/unipen/'\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "#import cv2\n",
    "\n",
    "from scipy.misc import imresize, imrotate, imsave \n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# display plots in this notebook\n",
    "%matplotlib inline\n",
    "\n",
    "# set display defaults\n",
    "plt.rcParams['figure.figsize'] = (10, 10)        # large images\n",
    "plt.rcParams['image.interpolation'] = 'nearest'  # don't interpolate: show square pixels\n",
    "plt.rcParams['image.cmap'] = 'gray'              # use grayscale output color heatmap\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#\n",
    "#Select a subset of characters\n",
    "#\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn import preprocessing\n",
    "\n",
    "def prepare_data_char_subset(X, y, char_select):\n",
    "    '''\n",
    "    Definition: Create train test data to model it\n",
    "        - Select cases\n",
    "        - shuffle\n",
    "        - separate train test\n",
    "        - encode target to dummy vars\n",
    "    Parameters:\n",
    "        X: images[n_images, x_size, y_size]\n",
    "        y: characters[n_images, 1]\n",
    "        char_select: list of characters selected\n",
    "        augmentation_fuction\n",
    "    \n",
    "    usage:\n",
    "        prepare_data_char_subset(X, y, set(['1','0']), augmentation_fuction = lambda: augmentate01(param1, param2=p2) )\n",
    "        \n",
    "    return:\n",
    "        X_train, y_train, X_test, y_test, labels_dictionary\n",
    "    '''\n",
    "    #Select cases\n",
    "    X_select = []\n",
    "    y_select = []\n",
    "    for row in xrange(X.shape[0]):\n",
    "        if y[row] in set(char_select):\n",
    "            X_select += [X[row,:,:]]\n",
    "            y_select += y[row]\n",
    "    X_select = np.array(X_select, dtype = np.float16)\n",
    "    y_select = np.array(y_select)\n",
    "    print 'Shape of selected cases:', X_select.shape, y_select.shape\n",
    "    \n",
    "    \n",
    "    #Shuffle\n",
    "    X_select, y_select = shuffle(X_select, y_select, random_state=0)\n",
    "    \n",
    "    #Reescale\n",
    "    X_select = X_select/255.\n",
    "    \n",
    "    #Recode target\n",
    "    decode_target={}\n",
    "    encode_target={}\n",
    "    for i,c in enumerate(char_select):\n",
    "        decode_target[i] = c \n",
    "        encode_target[c] = i\n",
    "    y_select = np.array([encode_target[y] for y in y_select])\n",
    "    \n",
    "\n",
    "    #Separate train test\n",
    "    X_train, X_test, y_train_ini, y_test_ini = train_test_split(X_select, y_select, test_size=0.20, random_state=42)\n",
    "    X_train = np.reshape(X_train,  (X_train.shape[0],1,X_train.shape[1],X_train.shape[2]))\n",
    "    X_test = np.reshape(X_test,  (X_test.shape[0],1,X_test.shape[1],X_test.shape[2]))\n",
    "    \n",
    "    print 'Train shape: ',X_train.shape, y_train_ini.shape\n",
    "    print 'Test shape: ',X_test.shape, y_test_ini.shape\n",
    "    print 'Num classes: ', len(set(y_train_ini))\n",
    "    print 'Classes:', set(y_train_ini)\n",
    "    \n",
    "    return X_train, y_train_ini, X_test, y_test_ini, decode_target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((46102, 64, 64), (46102,))\n",
      "Shape of selected cases: (29845, 64, 64) (29845,)\n",
      "Train shape:  (23876, 1, 64, 64) (23876,)\n",
      "Test shape:  (5969, 1, 64, 64) (5969,)\n",
      "Num classes:  26\n",
      "Classes: set([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25])\n",
      "((23876, 1, 64, 64), (5969, 1, 64, 64))\n"
     ]
    }
   ],
   "source": [
    "#Load data from hdf5\n",
    "import h5py\n",
    "\n",
    "hdf5_f = h5py.File(path + \"characters_base_64x64.hdf5\", mode='r')\n",
    "\n",
    "X = hdf5_f[\"X_curated_chars\"]\n",
    "y = hdf5_f[\"y_curated_chars\"]\n",
    "y_chars = np.array(y)\n",
    "print(X.shape, y.shape)\n",
    "\n",
    "char_select = ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l',\n",
    "               'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n",
    "\n",
    "X_train_l, y_train_l, X_test_l, y_test_l, decode_target_l = prepare_data_char_subset(X, y, char_select)\n",
    "print(X_train_l.shape, X_test_l.shape)\n",
    "\n",
    "hdf5_f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'a', 1: 'b', 2: 'c', 3: 'd', 4: 'e', 5: 'f', 6: 'g', 7: 'h', 8: 'i', 9: 'j', 10: 'k', 11: 'l', 12: 'm', 13: 'n', 14: 'o', 15: 'p', 16: 'q', 17: 'r', 18: 's', 19: 't', 20: 'u', 21: 'v', 22: 'w', 23: 'x', 24: 'y', 25: 'z'}\n",
      "[[ 0.          0.          0.          0.          0.          0.          0.\n",
      "   1.          1.          0.79589844  0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          1.          1.          1.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.        ]]\n",
      "[ 8  4 17 17 17]\n"
     ]
    }
   ],
   "source": [
    "print(decode_target_l)\n",
    "print(X_train_l[0,:,32,:])\n",
    "print(y_test_l[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Model creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n",
      "Using gpu device 0: GeForce GTX TITAN Black (CNMeM is disabled, cuDNN 5103)\n",
      "/usr/local/lib/python2.7/dist-packages/theano/sandbox/cuda/__init__.py:599: UserWarning: Your cuDNN version is more recent than the one Theano officially supports. If you see any problems, try updating Theano or downgrading cuDNN to version 5.\n",
      "  warnings.warn(warn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model 1...\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "#first model\n",
    "#\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Convolution2D, MaxPooling2D, Dense, Dropout, Flatten\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "print('Build model 1...')\n",
    "input_images = Input(shape=(1, 64, 64))\n",
    "\n",
    "c11 = Convolution2D(64, 3, 3, border_mode='same', activation='relu')(input_images)\n",
    "c12 = Convolution2D(64, 3, 3, border_mode='same', activation='relu')(c11)\n",
    "c1_mp = MaxPooling2D((2, 2))(c12)\n",
    "\n",
    "c21 = Convolution2D(128, 3, 3, border_mode='same', activation='relu')(c1_mp)\n",
    "c22 = Convolution2D(128, 3, 3, border_mode='same', activation='relu')(c21)\n",
    "c2_mp = MaxPooling2D((2, 2))(c22)\n",
    "\n",
    "c31 = Convolution2D(256, 3, 3, border_mode='same', activation='relu')(c2_mp)\n",
    "c32 = Convolution2D(256, 3, 3, border_mode='same', activation='relu')(c31)\n",
    "c33 = Convolution2D(256, 3, 3, border_mode='same', activation='relu')(c32)\n",
    "c3_mp = MaxPooling2D((2, 2))(c33)\n",
    "\n",
    "conv_out = Flatten()(c3_mp)\n",
    "\n",
    "dense1 = Dense(1024, activation='relu')(conv_out)\n",
    "after_dp1 = Dropout(0.5)(dense1)\n",
    "\n",
    "dense2 = Dense(1024, activation='relu')(after_dp1)\n",
    "after_dp2 = Dropout(0.5)(dense2)\n",
    "\n",
    "output = Dense(26, activation='softmax')(after_dp2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Data augmentation in keras\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range = 15,       # 15ยบ of random rotation\n",
    "    width_shift_range = 0.20,  # 20% of random translation width\n",
    "    height_shift_range = 0.20, # 20% of random translation height\n",
    "    shear_range = 0.15,        # 5ยบ of shear\n",
    "    zoom_range = 0.20)         # +- 20% of zoom \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "23876/23876 [==============================] - 82s - loss: 2.9439 - acc: 0.1551 - val_loss: 1.8418 - val_acc: 0.4681\n",
      "Epoch 2/100\n",
      "23876/23876 [==============================] - 87s - loss: 2.0718 - acc: 0.3909 - val_loss: 1.1987 - val_acc: 0.6740\n",
      "Epoch 3/100\n",
      "23876/23876 [==============================] - 87s - loss: 1.4779 - acc: 0.5665 - val_loss: 0.7731 - val_acc: 0.7859\n",
      "Epoch 4/100\n",
      "23876/23876 [==============================] - 87s - loss: 1.1737 - acc: 0.6458 - val_loss: 0.5113 - val_acc: 0.8529\n",
      "Epoch 5/100\n",
      "23876/23876 [==============================] - 87s - loss: 1.0030 - acc: 0.6990 - val_loss: 0.3951 - val_acc: 0.8849\n",
      "Epoch 6/100\n",
      "23876/23876 [==============================] - 87s - loss: 0.8909 - acc: 0.7316 - val_loss: 0.3584 - val_acc: 0.9000\n",
      "Epoch 7/100\n",
      "23876/23876 [==============================] - 87s - loss: 0.8081 - acc: 0.7549 - val_loss: 0.3347 - val_acc: 0.9060\n",
      "Epoch 8/100\n",
      "23876/23876 [==============================] - 87s - loss: 0.7377 - acc: 0.7781 - val_loss: 0.3009 - val_acc: 0.9136\n",
      "Epoch 9/100\n",
      "23876/23876 [==============================] - 87s - loss: 0.6893 - acc: 0.7915 - val_loss: 0.2789 - val_acc: 0.9187\n",
      "Epoch 10/100\n",
      "23876/23876 [==============================] - 87s - loss: 0.6601 - acc: 0.8023 - val_loss: 0.2683 - val_acc: 0.9253\n",
      "Epoch 11/100\n",
      "23876/23876 [==============================] - 87s - loss: 0.6277 - acc: 0.8074 - val_loss: 0.2466 - val_acc: 0.9265\n",
      "Epoch 12/100\n",
      "23876/23876 [==============================] - 87s - loss: 0.6066 - acc: 0.8181 - val_loss: 0.2398 - val_acc: 0.9281\n",
      "Epoch 13/100\n",
      "23876/23876 [==============================] - 87s - loss: 0.5792 - acc: 0.8205 - val_loss: 0.2259 - val_acc: 0.9293\n",
      "Epoch 14/100\n",
      "23876/23876 [==============================] - 87s - loss: 0.5641 - acc: 0.8285 - val_loss: 0.2334 - val_acc: 0.9316\n",
      "Epoch 15/100\n",
      "23876/23876 [==============================] - 87s - loss: 0.5380 - acc: 0.8350 - val_loss: 0.2029 - val_acc: 0.9375\n",
      "Epoch 16/100\n",
      "23876/23876 [==============================] - 87s - loss: 0.5287 - acc: 0.8399 - val_loss: 0.2013 - val_acc: 0.9375\n",
      "Epoch 17/100\n",
      "23876/23876 [==============================] - 87s - loss: 0.5108 - acc: 0.8434 - val_loss: 0.1998 - val_acc: 0.9370\n",
      "Epoch 18/100\n",
      "23876/23876 [==============================] - 87s - loss: 0.5122 - acc: 0.8419 - val_loss: 0.1933 - val_acc: 0.9405\n",
      "Epoch 19/100\n",
      "23876/23876 [==============================] - 87s - loss: 0.4910 - acc: 0.8481 - val_loss: 0.1839 - val_acc: 0.9444\n",
      "Epoch 20/100\n",
      "23876/23876 [==============================] - 87s - loss: 0.4783 - acc: 0.8535 - val_loss: 0.1808 - val_acc: 0.9439\n",
      "Epoch 21/100\n",
      "23876/23876 [==============================] - 87s - loss: 0.4781 - acc: 0.8527 - val_loss: 0.1775 - val_acc: 0.9471\n",
      "Epoch 22/100\n",
      "23876/23876 [==============================] - 87s - loss: 0.4617 - acc: 0.8575 - val_loss: 0.1761 - val_acc: 0.9474\n",
      "Epoch 23/100\n",
      "23876/23876 [==============================] - 87s - loss: 0.4596 - acc: 0.8572 - val_loss: 0.1749 - val_acc: 0.9476\n",
      "Epoch 24/100\n",
      "23876/23876 [==============================] - 87s - loss: 0.4558 - acc: 0.8614 - val_loss: 0.1697 - val_acc: 0.9502\n",
      "Epoch 25/100\n",
      "23876/23876 [==============================] - 87s - loss: 0.4426 - acc: 0.8651 - val_loss: 0.1672 - val_acc: 0.9481\n",
      "Epoch 26/100\n",
      "23876/23876 [==============================] - 87s - loss: 0.4408 - acc: 0.8641 - val_loss: 0.1643 - val_acc: 0.9491\n",
      "Epoch 27/100\n",
      "23876/23876 [==============================] - 87s - loss: 0.4209 - acc: 0.8704 - val_loss: 0.1686 - val_acc: 0.9469\n",
      "Epoch 28/100\n",
      "23876/23876 [==============================] - 87s - loss: 0.4284 - acc: 0.8665 - val_loss: 0.1667 - val_acc: 0.9489\n",
      "Epoch 29/100\n",
      "23876/23876 [==============================] - 87s - loss: 0.4254 - acc: 0.8692 - val_loss: 0.1610 - val_acc: 0.9497\n",
      "Epoch 30/100\n",
      "23876/23876 [==============================] - 87s - loss: 0.4251 - acc: 0.8702 - val_loss: 0.1609 - val_acc: 0.9499\n",
      "Epoch 31/100\n",
      "23876/23876 [==============================] - 87s - loss: 0.4099 - acc: 0.8754 - val_loss: 0.1587 - val_acc: 0.9512\n",
      "Epoch 32/100\n",
      "23876/23876 [==============================] - 87s - loss: 0.4071 - acc: 0.8722 - val_loss: 0.1571 - val_acc: 0.9511\n",
      "Epoch 33/100\n",
      "23876/23876 [==============================] - 87s - loss: 0.4051 - acc: 0.8745 - val_loss: 0.1525 - val_acc: 0.9541\n",
      "Epoch 34/100\n",
      "23876/23876 [==============================] - 87s - loss: 0.3953 - acc: 0.8767 - val_loss: 0.1561 - val_acc: 0.9528\n",
      "Epoch 35/100\n",
      "23876/23876 [==============================] - 87s - loss: 0.3953 - acc: 0.8782 - val_loss: 0.1501 - val_acc: 0.9538\n",
      "Epoch 36/100\n",
      "23876/23876 [==============================] - 87s - loss: 0.3917 - acc: 0.8804 - val_loss: 0.1483 - val_acc: 0.9539\n",
      "Epoch 37/100\n",
      "23876/23876 [==============================] - 87s - loss: 0.3837 - acc: 0.8794 - val_loss: 0.1505 - val_acc: 0.9541\n",
      "Epoch 38/100\n",
      "23876/23876 [==============================] - 87s - loss: 0.3793 - acc: 0.8836 - val_loss: 0.1465 - val_acc: 0.9541\n",
      "Epoch 39/100\n",
      "23876/23876 [==============================] - 87s - loss: 0.3752 - acc: 0.8807 - val_loss: 0.1443 - val_acc: 0.9539\n",
      "Epoch 40/100\n",
      "23876/23876 [==============================] - 87s - loss: 0.3780 - acc: 0.8831 - val_loss: 0.1492 - val_acc: 0.9553\n",
      "Epoch 41/100\n",
      "23876/23876 [==============================] - 87s - loss: 0.3763 - acc: 0.8837 - val_loss: 0.1450 - val_acc: 0.9569\n",
      "Epoch 42/100\n",
      "23876/23876 [==============================] - 87s - loss: 0.3784 - acc: 0.8822 - val_loss: 0.1451 - val_acc: 0.9563\n",
      "Epoch 43/100\n",
      "23876/23876 [==============================] - 87s - loss: 0.3738 - acc: 0.8845 - val_loss: 0.1417 - val_acc: 0.9571\n",
      "Epoch 44/100\n",
      "23876/23876 [==============================] - 87s - loss: 0.3535 - acc: 0.8899 - val_loss: 0.1373 - val_acc: 0.9568\n",
      "Epoch 45/100\n",
      "23876/23876 [==============================] - 87s - loss: 0.3659 - acc: 0.8840 - val_loss: 0.1436 - val_acc: 0.9549\n",
      "Epoch 46/100\n",
      "23876/23876 [==============================] - 87s - loss: 0.3595 - acc: 0.8896 - val_loss: 0.1369 - val_acc: 0.9596\n",
      "Epoch 47/100\n",
      "23876/23876 [==============================] - 87s - loss: 0.3611 - acc: 0.8856 - val_loss: 0.1360 - val_acc: 0.9579\n",
      "Epoch 48/100\n",
      "23876/23876 [==============================] - 87s - loss: 0.3564 - acc: 0.8881 - val_loss: 0.1445 - val_acc: 0.9539\n",
      "Epoch 49/100\n",
      "23876/23876 [==============================] - 87s - loss: 0.3494 - acc: 0.8919 - val_loss: 0.1342 - val_acc: 0.9596\n",
      "Epoch 50/100\n",
      "23876/23876 [==============================] - 87s - loss: 0.3476 - acc: 0.8916 - val_loss: 0.1372 - val_acc: 0.9579\n",
      "Epoch 51/100\n",
      "23876/23876 [==============================] - 87s - loss: 0.3515 - acc: 0.8914 - val_loss: 0.1398 - val_acc: 0.9578\n",
      "Epoch 52/100\n",
      "23876/23876 [==============================] - 87s - loss: 0.3481 - acc: 0.8901 - val_loss: 0.1372 - val_acc: 0.9568\n",
      "Epoch 53/100\n",
      "23876/23876 [==============================] - 87s - loss: 0.3470 - acc: 0.8916 - val_loss: 0.1400 - val_acc: 0.9568\n",
      "Epoch 54/100\n",
      "23876/23876 [==============================] - 87s - loss: 0.3392 - acc: 0.8945 - val_loss: 0.1355 - val_acc: 0.9581\n",
      "Epoch 55/100\n",
      "23876/23876 [==============================] - 87s - loss: 0.3304 - acc: 0.8962 - val_loss: 0.1334 - val_acc: 0.9581\n",
      "Epoch 56/100\n",
      "23876/23876 [==============================] - 87s - loss: 0.3389 - acc: 0.8955 - val_loss: 0.1387 - val_acc: 0.9564\n",
      "Epoch 57/100\n",
      "23876/23876 [==============================] - 87s - loss: 0.3249 - acc: 0.8965 - val_loss: 0.1348 - val_acc: 0.9568\n",
      "Epoch 58/100\n",
      "23876/23876 [==============================] - 87s - loss: 0.3363 - acc: 0.8964 - val_loss: 0.1348 - val_acc: 0.9574\n",
      "Epoch 59/100\n",
      "23876/23876 [==============================] - 87s - loss: 0.3328 - acc: 0.8969 - val_loss: 0.1310 - val_acc: 0.9598\n",
      "Epoch 60/100\n",
      "23876/23876 [==============================] - 87s - loss: 0.3344 - acc: 0.8963 - val_loss: 0.1342 - val_acc: 0.9583\n",
      "Epoch 61/100\n",
      "23876/23876 [==============================] - 87s - loss: 0.3320 - acc: 0.8957 - val_loss: 0.1323 - val_acc: 0.9600\n",
      "Epoch 62/100\n",
      "23876/23876 [==============================] - 87s - loss: 0.3240 - acc: 0.8983 - val_loss: 0.1324 - val_acc: 0.9590\n",
      "Epoch 63/100\n",
      "23876/23876 [==============================] - 87s - loss: 0.3182 - acc: 0.9022 - val_loss: 0.1271 - val_acc: 0.9613\n",
      "Epoch 64/100\n",
      "23876/23876 [==============================] - 87s - loss: 0.3171 - acc: 0.9004 - val_loss: 0.1330 - val_acc: 0.9576\n",
      "Epoch 65/100\n",
      "23876/23876 [==============================] - 87s - loss: 0.3185 - acc: 0.9004 - val_loss: 0.1312 - val_acc: 0.9596\n",
      "Epoch 66/100\n",
      "23876/23876 [==============================] - 87s - loss: 0.3261 - acc: 0.8993 - val_loss: 0.1315 - val_acc: 0.9581\n",
      "Epoch 67/100\n",
      "23876/23876 [==============================] - 87s - loss: 0.3213 - acc: 0.9003 - val_loss: 0.1267 - val_acc: 0.9615\n",
      "Epoch 68/100\n",
      "23876/23876 [==============================] - 87s - loss: 0.3225 - acc: 0.8976 - val_loss: 0.1286 - val_acc: 0.9596\n",
      "Epoch 69/100\n",
      "23876/23876 [==============================] - 87s - loss: 0.3158 - acc: 0.8982 - val_loss: 0.1263 - val_acc: 0.9608\n",
      "Epoch 70/100\n",
      "23876/23876 [==============================] - 87s - loss: 0.3122 - acc: 0.9029 - val_loss: 0.1266 - val_acc: 0.9605\n",
      "Epoch 71/100\n",
      "23876/23876 [==============================] - 87s - loss: 0.3097 - acc: 0.9009 - val_loss: 0.1235 - val_acc: 0.9610\n",
      "Epoch 72/100\n",
      "23876/23876 [==============================] - 87s - loss: 0.3134 - acc: 0.9013 - val_loss: 0.1266 - val_acc: 0.9603\n",
      "Epoch 73/100\n",
      "23876/23876 [==============================] - 87s - loss: 0.3134 - acc: 0.9016 - val_loss: 0.1217 - val_acc: 0.9623\n",
      "Epoch 74/100\n",
      "23876/23876 [==============================] - 87s - loss: 0.3106 - acc: 0.9034 - val_loss: 0.1273 - val_acc: 0.9600\n",
      "Epoch 75/100\n",
      "23876/23876 [==============================] - 87s - loss: 0.3068 - acc: 0.9025 - val_loss: 0.1245 - val_acc: 0.9631\n",
      "Epoch 76/100\n",
      "23876/23876 [==============================] - 87s - loss: 0.3079 - acc: 0.9030 - val_loss: 0.1262 - val_acc: 0.9623\n",
      "Epoch 77/100\n",
      "23876/23876 [==============================] - 87s - loss: 0.3080 - acc: 0.9042 - val_loss: 0.1223 - val_acc: 0.9626\n",
      "Epoch 78/100\n",
      "23876/23876 [==============================] - 87s - loss: 0.3033 - acc: 0.9039 - val_loss: 0.1260 - val_acc: 0.9588\n",
      "Epoch 79/100\n",
      "23876/23876 [==============================] - 87s - loss: 0.2980 - acc: 0.9063 - val_loss: 0.1248 - val_acc: 0.9621\n",
      "Epoch 80/100\n",
      "23876/23876 [==============================] - 87s - loss: 0.3058 - acc: 0.9043 - val_loss: 0.1228 - val_acc: 0.9625\n",
      "Epoch 81/100\n",
      "23876/23876 [==============================] - 87s - loss: 0.3051 - acc: 0.9040 - val_loss: 0.1246 - val_acc: 0.9636\n",
      "Epoch 82/100\n",
      "23876/23876 [==============================] - 87s - loss: 0.3050 - acc: 0.9044 - val_loss: 0.1233 - val_acc: 0.9636\n",
      "Epoch 83/100\n",
      "23876/23876 [==============================] - 87s - loss: 0.2943 - acc: 0.9069 - val_loss: 0.1222 - val_acc: 0.9621\n",
      "Epoch 84/100\n",
      "23876/23876 [==============================] - 87s - loss: 0.3044 - acc: 0.9029 - val_loss: 0.1225 - val_acc: 0.9640\n",
      "Epoch 85/100\n",
      "23876/23876 [==============================] - 87s - loss: 0.2929 - acc: 0.9071 - val_loss: 0.1214 - val_acc: 0.9631\n",
      "Epoch 86/100\n",
      "23876/23876 [==============================] - 87s - loss: 0.2990 - acc: 0.9043 - val_loss: 0.1235 - val_acc: 0.9625\n",
      "Epoch 87/100\n",
      "23876/23876 [==============================] - 87s - loss: 0.2878 - acc: 0.9075 - val_loss: 0.1210 - val_acc: 0.9633\n",
      "Epoch 88/100\n",
      "23876/23876 [==============================] - 87s - loss: 0.3029 - acc: 0.9032 - val_loss: 0.1221 - val_acc: 0.9643\n",
      "Epoch 89/100\n",
      "23876/23876 [==============================] - 87s - loss: 0.2984 - acc: 0.9055 - val_loss: 0.1202 - val_acc: 0.9647\n",
      "Epoch 90/100\n",
      "23876/23876 [==============================] - 87s - loss: 0.2974 - acc: 0.9066 - val_loss: 0.1210 - val_acc: 0.9630\n",
      "Epoch 91/100\n",
      "23876/23876 [==============================] - 87s - loss: 0.2952 - acc: 0.9059 - val_loss: 0.1216 - val_acc: 0.9636\n",
      "Epoch 92/100\n",
      "23876/23876 [==============================] - 87s - loss: 0.3025 - acc: 0.9047 - val_loss: 0.1226 - val_acc: 0.9630\n",
      "Epoch 93/100\n",
      "23876/23876 [==============================] - 87s - loss: 0.2890 - acc: 0.9097 - val_loss: 0.1194 - val_acc: 0.9631\n",
      "Epoch 94/100\n",
      "23876/23876 [==============================] - 87s - loss: 0.2933 - acc: 0.9076 - val_loss: 0.1214 - val_acc: 0.9636\n",
      "Epoch 95/100\n",
      "23876/23876 [==============================] - 87s - loss: 0.2918 - acc: 0.9091 - val_loss: 0.1196 - val_acc: 0.9626\n",
      "Epoch 96/100\n",
      "23876/23876 [==============================] - 87s - loss: 0.2917 - acc: 0.9073 - val_loss: 0.1216 - val_acc: 0.9626\n",
      "Epoch 97/100\n",
      "23876/23876 [==============================] - 87s - loss: 0.2854 - acc: 0.9077 - val_loss: 0.1180 - val_acc: 0.9640\n",
      "Epoch 98/100\n",
      "23876/23876 [==============================] - 87s - loss: 0.2928 - acc: 0.9069 - val_loss: 0.1200 - val_acc: 0.9648\n",
      "Epoch 99/100\n",
      "23876/23876 [==============================] - 87s - loss: 0.2900 - acc: 0.9104 - val_loss: 0.1212 - val_acc: 0.9631\n",
      "Epoch 100/100\n",
      "23876/23876 [==============================] - 87s - loss: 0.2909 - acc: 0.9058 - val_loss: 0.1201 - val_acc: 0.9630\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# Create the model\n",
    "model_l = Model(input=input_images, output=output)\n",
    "\n",
    "#Fit model  on batches with real-time data augmentation:\n",
    "sgd = SGD(lr=0.01, decay=0.001, momentum=0.9, nesterov=True)\n",
    "model_l.compile(loss='sparse_categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "\n",
    "hist_l_1 = model_l.fit_generator(datagen.flow(X_train_l, y_train_l, batch_size=128),\n",
    "                    samples_per_epoch=len(X_train_l), nb_epoch=100, \n",
    "                    validation_data=(X_test_l, y_test_l))\n",
    "\n",
    "print 'Done!'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accuracy for UNIPEN lower case: 96.3%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f1174297790>]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAlcAAAJPCAYAAABRvvFyAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f119c3187d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(hist_l_1.history['val_acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Save model\n",
    "path_models = '/home/jorge/data/tesis/handwriting/p01_read_character/'\n",
    "\n",
    "model_name = 'unipen_lowercase_01'\n",
    "\n",
    "json_string = model_l.to_json()\n",
    "open(path_models + 'models/mdl_' + model_name + '.json', 'w').write(json_string)\n",
    "model_l.save_weights(path_models + 'models/w_' + model_name + '.h5', overwrite=True)\n",
    "\n",
    "# Save decode_target\n",
    "import pickle\n",
    "pickle.dump( decode_target_l, open( path_models + \"models/unipen_decode_target_lowercase.p\", \"wb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Use the previous created model to score NIST database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Load model\n",
    "from keras.models import model_from_json\n",
    "\n",
    "model_name = 'unipen_lowercase_01'\n",
    "\n",
    "model_l = model_from_json(open(path_models + 'models/mdl_' + model_name + '.json').read())\n",
    "model_l.load_weights(path_models + 'models/w_' + model_name + '.h5')\n",
    "\n",
    "# Load the dictionary back from the pickle file.\n",
    "import pickle\n",
    "decode_target = pickle.load( open(path_models + \"models/unipen_decode_target_lowercase.p\", \"rb\" ) )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'a', 1: 'b', 2: 'c', 3: 'd', 4: 'e', 5: 'f', 6: 'g', 7: 'h', 8: 'i', 9: 'j', 10: 'k', 11: 'l', 12: 'm', 13: 'n', 14: 'o', 15: 'p', 16: 'q', 17: 'r', 18: 's', 19: 't', 20: 'u', 21: 'v', 22: 'w', 23: 'x', 24: 'y', 25: 'z'}\n"
     ]
    }
   ],
   "source": [
    "#Read NIST dataset \n",
    "\n",
    "#Read NIST database\n",
    "path_NIST='/home/jorge/data/tesis/handwriting/databases/NIST/by_class/'\n",
    "\n",
    "\n",
    "char_list_lo = ['61','62','63','64','65','66','67','68','69','6a','6b','6c','6d'\n",
    "               ,'6e','6f','70','71','72','73','74','75','76','77','78','79','7a']\n",
    "\n",
    "\n",
    "decode_lo={}\n",
    "encode_lo={}\n",
    "for i , c in enumerate(char_list_lo):\n",
    "    char = str(unichr(int(c,16)))\n",
    "    decode_lo[i] = char\n",
    "    encode_lo[char] = i\n",
    "print decode_lo               \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Generator of list of files in a folder and subfolders\n",
    "import os\n",
    "import shutil\n",
    "import fnmatch\n",
    "\n",
    "def gen_find(filepat,top):\n",
    "    for path, dirlist, filelist in os.walk(top):\n",
    "        for name in fnmatch.filter(filelist,filepat):\n",
    "            yield os.path.join(path,name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12000, 64, 64)\n",
      "(12000,)\n"
     ]
    }
   ],
   "source": [
    "hsf='/hsf_4/'\n",
    "\n",
    "#Read test dataset\n",
    "X_NIST_lo=[]\n",
    "y_NIST_lo=[]\n",
    "for char in char_list_lo:\n",
    "    letter = str(unichr(int(char,16)))\n",
    "    images_list = gen_find(\"*.png\", path_NIST+char+hsf) \n",
    "    for img_name in images_list:\n",
    "        img = plt.imread(img_name)\n",
    "        \n",
    "        #Transform\n",
    "        img = img[32:96,32:96,0]\n",
    "        \n",
    "        X_NIST_lo += [img]\n",
    "        y_NIST_lo += [encode_lo[letter]]\n",
    "\n",
    "X_NIST_lo = 1. - np.array(X_NIST_lo)\n",
    "y_NIST_lo = np.array(y_NIST_lo)\n",
    "        \n",
    "print(X_NIST_lo.shape)\n",
    "print(y_NIST_lo.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  1.  1.  1.  1.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  1.  1.  1.  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n"
     ]
    }
   ],
   "source": [
    "print(X_NIST_lo[0,32,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12000, 26)\n"
     ]
    }
   ],
   "source": [
    "y_pred = model_l.predict(X_NIST_lo.reshape((12000, 1, 64, 64)), batch_size=128)\n",
    "print(y_pred.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.715166666667\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "print(accuracy_score(y_NIST_lo, np.argmax(y_pred, axis=1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Accuracy  for NIST database (2nd ed) lower case: 71.5%\n",
    "    - The low score can be explained because the NIST database is binary (pixels only 0 or 1).\n",
    "    - We need to train the same architecture over the NIST database in order to obtain a better comparation to others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
